# Should we use OpenAI's embedding scheme and LLM?
openai: true

# If not, what embedding scheme and LLM should we use?
embeddings:
  model: hkunlp/instructor-large

llm: ctransformers

# Should we download this model for offline use? Keep in mind file and GPU size.
download: false

# There are options for models:
ctransformers:
  model: TheBloke/Wizard-Vicuna-7B-Uncensored-GGML
  model_file: Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin
  model_type: llama
  config:
    context_length: 1024

huggingface:
  model: TheBloke/Wizard-Vicuna-7B-Uncensored-HF
  pipeline_kwargs:
    max_new_tokens: 256

gptq:
  model: TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ
  model_file: Wizard-Vicuna-7B-Uncensored-GPTQ-4bit-128g.no-act-order.safetensors
  pipeline_kwargs:
    max_new_tokens: 256

# How should we store the vector database?
chroma:
  persist_directory: db
  chroma_db_impl: duckdb+parquet
  anonymized_telemetry: false
  
# Where should we host the user interface?
host: localhost
port: 5000
auth: false

retriever:
  search_kwargs:
    k: 4